diff -Naur a/src/pmemd/src/cuda/kBNL.h b/src/pmemd/src/cuda/kBNL.h
--- a/src/pmemd/src/cuda/kBNL.h	2020-10-01 12:47:20.079767250 +0200
+++ b/src/pmemd/src/cuda/kBNL.h	2020-10-01 13:06:07.019866636 +0200
@@ -90,26 +90,26 @@
   // The union shNlRecord is used for various pieces of information, both floats
   // and unsigned integers, and in only one critical case is it ever used to
   // interpret one as the other.
-  while (__shfl(shNlRecord.u, NEIGHBOR_CELLS + 2) < cSim.NLRecords) {
+  while (__shfl_sync(0xffffffff, shNlRecord.u, NEIGHBOR_CELLS + 2) < cSim.NLRecords) {
     unsigned int tgx = threadIdx.x & GRID_BITS_MASK;
 
     // Read NLRecord information
-    unsigned int pos1 = __shfl(shNlRecord.u, NEIGHBOR_CELLS + 2);
+    unsigned int pos1 = __shfl_sync(0xffffffff, shNlRecord.u, NEIGHBOR_CELLS + 2);
     if (tgx < 16) {
       shNlRecord.u = cSim.pNLRecord[pos1].array[tgx];
     }
 
     // Calculate Exclusion/neighbor list space required
-    int atomOffset = __shfl(shNlRecord.u, NEIGHBOR_CELLS + 1) >> NLRECORD_YOFFSET_SHIFT;
-    uint2 homeCell = cSim.pNLNonbondCellStartEnd[__shfl(shNlRecord.u, NEIGHBOR_CELLS)];
+    int atomOffset = __shfl_sync(0xffffffff, shNlRecord.u, NEIGHBOR_CELLS + 1) >> NLRECORD_YOFFSET_SHIFT;
+    uint2 homeCell = cSim.pNLNonbondCellStartEnd[__shfl_sync(0xffffffff, shNlRecord.u, NEIGHBOR_CELLS)];
     if (tgx == (NEIGHBOR_CELLS + 4)) {
       shNlRecord.u = homeCell.x;
     }
     if (tgx == (NEIGHBOR_CELLS + 5)) {
       shNlRecord.u = homeCell.y;
     }
-    int ysize = max(0, (int)(__shfl(shNlRecord.u, NEIGHBOR_CELLS + 5) -
-                             __shfl(shNlRecord.u, NEIGHBOR_CELLS + 4) -
+    int ysize = max(0, (int)(__shfl_sync(0xffffffff, shNlRecord.u, NEIGHBOR_CELLS + 5) -
+                             __shfl_sync(0xffffffff, shNlRecord.u, NEIGHBOR_CELLS + 4) -
                              atomOffset * cSim.NLAtomsPerWarp));
     if (ysize > 0) {
       ysize = 1 + max(0, ysize - 1) / (cSim.NLAtomsPerWarp * cSim.NLYDivisor);
@@ -119,9 +119,9 @@
     // cSim.pNLRecord[pos1].array[tgx] for tgx < 16, homeCell.x for tgx == 18, and homeCell.y
     // for tgx == 19.  The value broadcast for setting cells on all threads is going to be
     // for tgx == 15, which is the final cell with which .
-    unsigned int cells = __shfl(shNlRecord.u, NEIGHBOR_CELLS + 1) & NLRECORD_CELL_COUNT_MASK;
+    unsigned int cells = __shfl_sync(0xffffffff, shNlRecord.u, NEIGHBOR_CELLS + 1) & NLRECORD_CELL_COUNT_MASK;
     if (tgx < cells) {
-      uint2 cell = cSim.pNLNonbondCellStartEnd[__shfl(shNlRecord.u, tgx) >>
+      uint2 cell = cSim.pNLNonbondCellStartEnd[__shfl_sync(0xffffffff, shNlRecord.u, tgx) >>
                                                NLRECORD_CELL_SHIFT];
       shCell.x = cell.x;
       shCell.y = cell.y;
@@ -157,13 +157,13 @@
     }
 
     // Generate actual neighbor list entry
-    uint ypos = __shfl(shNlRecord.u, NEIGHBOR_CELLS + 4) +
-                ((__shfl(shNlRecord.u,
+    uint ypos = __shfl_sync(0xffffffff, shNlRecord.u, NEIGHBOR_CELLS + 4) +
+                ((__shfl_sync(0xffffffff, shNlRecord.u,
                          NEIGHBOR_CELLS + 1) >> NLRECORD_YOFFSET_SHIFT) * cSim.NLAtomsPerWarp);
-    while (ypos < __shfl(shNlRecord.u, NEIGHBOR_CELLS + 5)) {
+    while (ypos < __shfl_sync(0xffffffff, shNlRecord.u, NEIGHBOR_CELLS + 5)) {
 
       // Calculate y bounds and set to calculate homecell interaction
-      uint ymax = min(ypos + cSim.NLAtomsPerWarp, __shfl(shNlRecord.u, NEIGHBOR_CELLS + 5));
+      uint ymax = min(ypos + cSim.NLAtomsPerWarp, __shfl_sync(0xffffffff, shNlRecord.u, NEIGHBOR_CELLS + 5));
       psWarp->nlEntry.NL.ypos = ypos;
       psWarp->nlEntry.NL.ymax = (ymax << NLENTRY_YMAX_SHIFT) | 1;
       psWarp->nlEntry.NL.xatoms = 0;
@@ -202,23 +202,23 @@
       // Calculate bounding box on SM 2.0 and up
       volatile float bmin = (index < ymax) ? 0.5f * xi :  999999.0f;
       volatile float bmax = (index < ymax) ? 0.5f * xi : -999999.0f;
-      bmin = min(__shfl(bmin, tgx ^ 1), bmin);
-      bmin = min(__shfl(bmin, tgx ^ 2), bmin);
-      bmin = min(__shfl(bmin, tgx ^ 4), bmin);
+      bmin = min(__shfl_sync(0xffffffff, bmin, tgx ^ 1), bmin);
+      bmin = min(__shfl_sync(0xffffffff, bmin, tgx ^ 2), bmin);
+      bmin = min(__shfl_sync(0xffffffff, bmin, tgx ^ 4), bmin);
 #if (PME_ATOMS_PER_WARP >= 16)
-      bmin = min(__shfl(bmin, tgx ^ 8), bmin);
+      bmin = min(__shfl_sync(0xffffffff, bmin, tgx ^ 8), bmin);
 #endif
 #if (PME_ATOMS_PER_WARP == 32)
-      bmin = min(__shfl(bmin, tgx ^ 16), bmin);
+      bmin = min(__shfl_sync(0xffffffff, bmin, tgx ^ 16), bmin);
 #endif
-      bmax = max(__shfl(bmax, tgx ^ 1), bmax);
-      bmax = max(__shfl(bmax, tgx ^ 2), bmax);
-      bmax = max(__shfl(bmax, tgx ^ 4), bmax);
+      bmax = max(__shfl_sync(0xffffffff, bmax, tgx ^ 1), bmax);
+      bmax = max(__shfl_sync(0xffffffff, bmax, tgx ^ 2), bmax);
+      bmax = max(__shfl_sync(0xffffffff, bmax, tgx ^ 4), bmax);
 #if (PME_ATOMS_PER_WARP >= 16)
-      bmax = max(__shfl(bmax, tgx ^ 8), bmax);
+      bmax = max(__shfl_sync(0xffffffff, bmax, tgx ^ 8), bmax);
 #endif
 #if (PME_ATOMS_PER_WARP == 32)
-      bmax = max(__shfl(bmax, tgx ^ 16), bmax);
+      bmax = max(__shfl_sync(0xffffffff, bmax, tgx ^ 16), bmax);
 #endif
       if (tgx == NEIGHBOR_CELLS + 6) {
         shNlRecord.f = bmax + bmin;
@@ -228,23 +228,23 @@
       }
       bmin = (index < ymax) ? 0.5f * yi :  999999.0f;
       bmax = (index < ymax) ? 0.5f * yi : -999999.0f;
-      bmin = min(__shfl(bmin, tgx ^ 1), bmin);
-      bmin = min(__shfl(bmin, tgx ^ 2), bmin);
-      bmin = min(__shfl(bmin, tgx ^ 4), bmin);
+      bmin = min(__shfl_sync(0xffffffff, bmin, tgx ^ 1), bmin);
+      bmin = min(__shfl_sync(0xffffffff, bmin, tgx ^ 2), bmin);
+      bmin = min(__shfl_sync(0xffffffff, bmin, tgx ^ 4), bmin);
 #if (PME_ATOMS_PER_WARP >= 16)
-      bmin = min(__shfl(bmin, tgx ^ 8), bmin);
+      bmin = min(__shfl_sync(0xffffffff, bmin, tgx ^ 8), bmin);
 #endif
 #if (PME_ATOMS_PER_WARP == 32)
-      bmin = min(__shfl(bmin, tgx ^ 16), bmin);
+      bmin = min(__shfl_sync(0xffffffff, bmin, tgx ^ 16), bmin);
 #endif
-      bmax = max(__shfl(bmax, tgx ^ 1), bmax);
-      bmax = max(__shfl(bmax, tgx ^ 2), bmax);
-      bmax = max(__shfl(bmax, tgx ^ 4), bmax);
+      bmax = max(__shfl_sync(0xffffffff, bmax, tgx ^ 1), bmax);
+      bmax = max(__shfl_sync(0xffffffff, bmax, tgx ^ 2), bmax);
+      bmax = max(__shfl_sync(0xffffffff, bmax, tgx ^ 4), bmax);
 #if (PME_ATOMS_PER_WARP >= 16)
-      bmax = max(__shfl(bmax, tgx ^ 8), bmax);
+      bmax = max(__shfl_sync(0xffffffff, bmax, tgx ^ 8), bmax);
 #endif
 #if (PME_ATOMS_PER_WARP == 32)
-      bmax = max(__shfl(bmax, tgx ^ 16), bmax);
+      bmax = max(__shfl_sync(0xffffffff, bmax, tgx ^ 16), bmax);
 #endif
       if (tgx == NEIGHBOR_CELLS + 8) {
         shNlRecord.f = bmax + bmin;
@@ -254,23 +254,23 @@
       }
       bmin = (index < ymax) ? 0.5f * zi :  999999.0f;
       bmax = (index < ymax) ? 0.5f * zi : -999999.0f;
-      bmin = min(__shfl(bmin, tgx ^ 1), bmin);
-      bmin = min(__shfl(bmin, tgx ^ 2), bmin);
-      bmin = min(__shfl(bmin, tgx ^ 4), bmin);
+      bmin = min(__shfl_sync(0xffffffff, bmin, tgx ^ 1), bmin);
+      bmin = min(__shfl_sync(0xffffffff, bmin, tgx ^ 2), bmin);
+      bmin = min(__shfl_sync(0xffffffff, bmin, tgx ^ 4), bmin);
 #if (PME_ATOMS_PER_WARP >= 16)
-      bmin = min(__shfl(bmin, tgx ^ 8), bmin);
+      bmin = min(__shfl_sync(0xffffffff, bmin, tgx ^ 8), bmin);
 #endif
 #if (PME_ATOMS_PER_WARP == 32)
-      bmin = min(__shfl(bmin, tgx ^ 16), bmin);
+      bmin = min(__shfl_sync(0xffffffff, bmin, tgx ^ 16), bmin);
 #endif
-      bmax = max(__shfl(bmax, tgx ^ 1), bmax);
-      bmax = max(__shfl(bmax, tgx ^ 2), bmax);
-      bmax = max(__shfl(bmax, tgx ^ 4), bmax);
+      bmax = max(__shfl_sync(0xffffffff, bmax, tgx ^ 1), bmax);
+      bmax = max(__shfl_sync(0xffffffff, bmax, tgx ^ 2), bmax);
+      bmax = max(__shfl_sync(0xffffffff, bmax, tgx ^ 4), bmax);
 #if (PME_ATOMS_PER_WARP >= 16)
-      bmax = max(__shfl(bmax, tgx ^ 8), bmax);
+      bmax = max(__shfl_sync(0xffffffff, bmax, tgx ^ 8), bmax);
 #endif
 #if (PME_ATOMS_PER_WARP == 32)
-      bmax = max(__shfl(bmax, tgx ^ 16), bmax);
+      bmax = max(__shfl_sync(0xffffffff, bmax, tgx ^ 16), bmax);
 #endif
       if (tgx == NEIGHBOR_CELLS + 10) {
         shNlRecord.f = bmax + bmin;
@@ -306,16 +306,16 @@
         }
         totalExclusions += count;
       }
-      minExclusion = min(__shfl(minExclusion, tgx ^ 1), minExclusion);
-      minExclusion = min(__shfl(minExclusion, tgx ^ 2), minExclusion);
-      minExclusion = min(__shfl(minExclusion, tgx ^ 4), minExclusion);
-      minExclusion = min(__shfl(minExclusion, tgx ^ 8), minExclusion);
-      minExclusion = min(__shfl(minExclusion, tgx ^ 16), minExclusion);
-      maxExclusion = max(__shfl(maxExclusion, tgx ^ 1), maxExclusion);
-      maxExclusion = max(__shfl(maxExclusion, tgx ^ 2), maxExclusion);
-      maxExclusion = max(__shfl(maxExclusion, tgx ^ 4), maxExclusion);
-      maxExclusion = max(__shfl(maxExclusion, tgx ^ 8), maxExclusion);
-      maxExclusion = max(__shfl(maxExclusion, tgx ^ 16), maxExclusion);
+      minExclusion = min(__shfl_sync(0xffffffff, minExclusion, tgx ^ 1), minExclusion);
+      minExclusion = min(__shfl_sync(0xffffffff, minExclusion, tgx ^ 2), minExclusion);
+      minExclusion = min(__shfl_sync(0xffffffff, minExclusion, tgx ^ 4), minExclusion);
+      minExclusion = min(__shfl_sync(0xffffffff, minExclusion, tgx ^ 8), minExclusion);
+      minExclusion = min(__shfl_sync(0xffffffff, minExclusion, tgx ^ 16), minExclusion);
+      maxExclusion = max(__shfl_sync(0xffffffff, maxExclusion, tgx ^ 1), maxExclusion);
+      maxExclusion = max(__shfl_sync(0xffffffff, maxExclusion, tgx ^ 2), maxExclusion);
+      maxExclusion = max(__shfl_sync(0xffffffff, maxExclusion, tgx ^ 4), maxExclusion);
+      maxExclusion = max(__shfl_sync(0xffffffff, maxExclusion, tgx ^ 8), maxExclusion);
+      maxExclusion = max(__shfl_sync(0xffffffff, maxExclusion, tgx ^ 16), maxExclusion);
       if (tgx == NEIGHBOR_CELLS + 12) {
         shNlRecord.u = minExclusion;
       }
@@ -333,7 +333,7 @@
       while (cpos < cells) {
 
         // Check for home cell
-        shCellID = __shfl(shNlRecord.u, cpos) & NLRECORD_CELL_TYPE_MASK;
+        shCellID = __shfl_sync(0xffffffff, shNlRecord.u, cpos) & NLRECORD_CELL_TYPE_MASK;
         uint xpos;
 
         // Cell 0 always starts along force matrix diagonal
@@ -368,14 +368,14 @@
           xpos = ypos + cSim.NLAtomsPerWarp;
         }
         else {
-          xpos = __shfl(shCell.x, cpos);
+          xpos = __shfl_sync(0xffffffff, shCell.x, cpos);
         }
 
         // Read x atoms
-        while (xpos < __shfl(shCell.y, cpos)) {
+        while (xpos < __shfl_sync(0xffffffff, shCell.y, cpos)) {
 
           // Calculate number of atoms in this iteration
-          uint xmax = min(xpos + GRID, __shfl(shCell.y, cpos)) - xpos;
+          uint xmax = min(xpos + GRID, __shfl_sync(0xffffffff, shCell.y, cpos)) - xpos;
           float sAtomx;
           float sAtomy;
           float sAtomz;
@@ -419,12 +419,12 @@
 #endif
           // Bounding box test on SM 2.0+
           float trivialCut2 = 0.5625f * cutPlusSkin2;
-          float bxc = __shfl(shNlRecord.f, NEIGHBOR_CELLS + 6);
-          float bxr = __shfl(shNlRecord.f, NEIGHBOR_CELLS + 7);
-          float byc = __shfl(shNlRecord.f, NEIGHBOR_CELLS + 8);
-          float byr = __shfl(shNlRecord.f, NEIGHBOR_CELLS + 9);
-          float bzc = __shfl(shNlRecord.f, NEIGHBOR_CELLS + 10);
-          float bzr = __shfl(shNlRecord.f, NEIGHBOR_CELLS + 11);
+          float bxc = __shfl_sync(0xffffffff, shNlRecord.f, NEIGHBOR_CELLS + 6);
+          float bxr = __shfl_sync(0xffffffff, shNlRecord.f, NEIGHBOR_CELLS + 7);
+          float byc = __shfl_sync(0xffffffff, shNlRecord.f, NEIGHBOR_CELLS + 8);
+          float byr = __shfl_sync(0xffffffff, shNlRecord.f, NEIGHBOR_CELLS + 9);
+          float bzc = __shfl_sync(0xffffffff, shNlRecord.f, NEIGHBOR_CELLS + 10);
+          float bzr = __shfl_sync(0xffffffff, shNlRecord.f, NEIGHBOR_CELLS + 11);
           float tx = fabs(sAtomx - bxc);
           float ty = fabs(sAtomy - byc);
           float tz = fabs(sAtomz - bzc);
@@ -477,9 +477,9 @@
             }
 #endif
 
-            float ax = __shfl(sAtomx, pos);
-            float ay = __shfl(sAtomy, pos);
-            float az = __shfl(sAtomz, pos);
+            float ax = __shfl_sync(0xffffffff, sAtomx, pos);
+            float ay = __shfl_sync(0xffffffff, sAtomy, pos);
+            float az = __shfl_sync(0xffffffff, sAtomz, pos);
             int pred = 0;
             if (pos >= 0) {
               float dx = xi - ax;
@@ -541,29 +541,29 @@
               psWarp->nlEntry.NL.xatoms += GRID;
 
               // Clear used bits from bpred
-              bpred &= __shfl(bpred, tgx ^ 1);
-              bpred &= __shfl(bpred, tgx ^ 2);
-              bpred &= __shfl(bpred, tgx ^ 4);
-              bpred &= __shfl(bpred, tgx ^ 8);
-              bpred &= __shfl(bpred, tgx ^ 16);
+              bpred &= __shfl_sync(0xffffffff, bpred, tgx ^ 1);
+              bpred &= __shfl_sync(0xffffffff, bpred, tgx ^ 2);
+              bpred &= __shfl_sync(0xffffffff, bpred, tgx ^ 4);
+              bpred &= __shfl_sync(0xffffffff, bpred, tgx ^ 8);
+              bpred &= __shfl_sync(0xffffffff, bpred, tgx ^ 16);
 
               // Reduce minatom and maxatom
-              minAtom = min(minAtom, __shfl(minAtom, tgx ^ 1));
-              minAtom = min(minAtom, __shfl(minAtom, tgx ^ 2));
-              minAtom = min(minAtom, __shfl(minAtom, tgx ^ 4));
-              minAtom = min(minAtom, __shfl(minAtom, tgx ^ 8));
-              minAtom = min(minAtom, __shfl(minAtom, tgx ^ 16));
-              maxAtom = max(maxAtom, __shfl(maxAtom, tgx ^ 1));
-              maxAtom = max(maxAtom, __shfl(maxAtom, tgx ^ 2));
-              maxAtom = max(maxAtom, __shfl(maxAtom, tgx ^ 4));
-              maxAtom = max(maxAtom, __shfl(maxAtom, tgx ^ 8));
-              maxAtom = max(maxAtom, __shfl(maxAtom, tgx ^ 16));
+              minAtom = min(minAtom, __shfl_sync(0xffffffff, minAtom, tgx ^ 1));
+              minAtom = min(minAtom, __shfl_sync(0xffffffff, minAtom, tgx ^ 2));
+              minAtom = min(minAtom, __shfl_sync(0xffffffff, minAtom, tgx ^ 4));
+              minAtom = min(minAtom, __shfl_sync(0xffffffff, minAtom, tgx ^ 8));
+              minAtom = min(minAtom, __shfl_sync(0xffffffff, minAtom, tgx ^ 16));
+              maxAtom = max(maxAtom, __shfl_sync(0xffffffff, maxAtom, tgx ^ 1));
+              maxAtom = max(maxAtom, __shfl_sync(0xffffffff, maxAtom, tgx ^ 2));
+              maxAtom = max(maxAtom, __shfl_sync(0xffffffff, maxAtom, tgx ^ 4));
+              maxAtom = max(maxAtom, __shfl_sync(0xffffffff, maxAtom, tgx ^ 8));
+              maxAtom = max(maxAtom, __shfl_sync(0xffffffff, maxAtom, tgx ^ 16));
 
               // Search for y atom exclusions matching any x atom all at once (this should
               // reduce exclusion tests by a factor of approximately 100 overall).
               // But first, rule out skipping exclusion test.
-              uint minExclusion = __shfl(shNlRecord.u, NEIGHBOR_CELLS + 12);
-              uint maxExclusion = __shfl(shNlRecord.u, NEIGHBOR_CELLS + 13);
+              uint minExclusion = __shfl_sync(0xffffffff, shNlRecord.u, NEIGHBOR_CELLS + 12);
+              uint maxExclusion = __shfl_sync(0xffffffff, shNlRecord.u, NEIGHBOR_CELLS + 13);
               psWarp->exclusionMask[tgx] = 0;
               if ((minAtom <= maxExclusion) && (maxAtom >= minExclusion)) {
                 uint atom = (psWarp->atomList[tgx] >> NLATOM_CELL_SHIFT);
@@ -632,16 +632,16 @@
       if (atoms > 0) {
 
         // Reduce minatom and maxatom
-        minAtom = min(minAtom, __shfl(minAtom, tgx ^ 1));
-        minAtom = min(minAtom, __shfl(minAtom, tgx ^ 2));
-        minAtom = min(minAtom, __shfl(minAtom, tgx ^ 4));
-        minAtom = min(minAtom, __shfl(minAtom, tgx ^ 8));
-        minAtom = min(minAtom, __shfl(minAtom, tgx ^ 16));
-        maxAtom = max(maxAtom, __shfl(maxAtom, tgx ^ 1));
-        maxAtom = max(maxAtom, __shfl(maxAtom, tgx ^ 2));
-        maxAtom = max(maxAtom, __shfl(maxAtom, tgx ^ 4));
-        maxAtom = max(maxAtom, __shfl(maxAtom, tgx ^ 8));
-        maxAtom = max(maxAtom, __shfl(maxAtom, tgx ^ 16));
+        minAtom = min(minAtom, __shfl_sync(0xffffffff, minAtom, tgx ^ 1));
+        minAtom = min(minAtom, __shfl_sync(0xffffffff, minAtom, tgx ^ 2));
+        minAtom = min(minAtom, __shfl_sync(0xffffffff, minAtom, tgx ^ 4));
+        minAtom = min(minAtom, __shfl_sync(0xffffffff, minAtom, tgx ^ 8));
+        minAtom = min(minAtom, __shfl_sync(0xffffffff, minAtom, tgx ^ 16));
+        maxAtom = max(maxAtom, __shfl_sync(0xffffffff, maxAtom, tgx ^ 1));
+        maxAtom = max(maxAtom, __shfl_sync(0xffffffff, maxAtom, tgx ^ 2));
+        maxAtom = max(maxAtom, __shfl_sync(0xffffffff, maxAtom, tgx ^ 4));
+        maxAtom = max(maxAtom, __shfl_sync(0xffffffff, maxAtom, tgx ^ 8));
+        maxAtom = max(maxAtom, __shfl_sync(0xffffffff, maxAtom, tgx ^ 16));
         if (tgx < atoms) {
           cSim.pNLAtomList[psWarp->offset + tgx] = psWarp->atomList[tgx];
         }
@@ -652,8 +652,8 @@
 
         // Search for y atom exclusions matching any x atom all at once (this should
         // reduce exclusion tests by a factor of approximately 100 overall)
-        uint minExclusion = __shfl(shNlRecord.u, NEIGHBOR_CELLS + 12);
-        uint maxExclusion = __shfl(shNlRecord.u, NEIGHBOR_CELLS + 13);
+        uint minExclusion = __shfl_sync(0xffffffff, shNlRecord.u, NEIGHBOR_CELLS + 12);
+        uint maxExclusion = __shfl_sync(0xffffffff, shNlRecord.u, NEIGHBOR_CELLS + 13);
         psWarp->exclusionMask[tgx] = 0;
         if ((tgx < atoms) && ((minAtom <= maxExclusion) && (maxAtom >= minExclusion))) {
           unsigned int atom = (psWarp->atomList[tgx] >> NLATOM_CELL_SHIFT);
diff -Naur a/src/pmemd/src/cuda/kBWU_cimp.h b/src/pmemd/src/cuda/kBWU_cimp.h
--- a/src/pmemd/src/cuda/kBWU_cimp.h	2020-10-01 12:47:20.192767259 +0200
+++ b/src/pmemd/src/cuda/kBWU_cimp.h	2020-10-01 13:06:07.029866637 +0200
@@ -138,11 +138,11 @@
     ecimp = llrint(ENERGYSCALE * impDihedral.x * (phi - impDihedral.y) *
                    (phi - impDihedral.y));
   }
-  ecimp += __shfl(ecimp, tgx + 16);
-  ecimp += __shfl(ecimp, tgx +  8);
-  ecimp += __shfl(ecimp, tgx +  4);
-  ecimp += __shfl(ecimp, tgx +  2);
-  ecimp += __shfl(ecimp, tgx +  1);
+  ecimp += __shfl_sync(0xffffffff, ecimp, tgx + 16);
+  ecimp += __shfl_sync(0xffffffff, ecimp, tgx +  8);
+  ecimp += __shfl_sync(0xffffffff, ecimp, tgx +  4);
+  ecimp += __shfl_sync(0xffffffff, ecimp, tgx +  2);
+  ecimp += __shfl_sync(0xffffffff, ecimp, tgx +  1);
   if (tgx == 0) {
     nrgACC[CIMP_EACC_OFFSET + warpIdx] += ecimp;
   }
diff -Naur a/src/pmemd/src/cuda/kBWU_cmap.h b/src/pmemd/src/cuda/kBWU_cmap.h
--- a/src/pmemd/src/cuda/kBWU_cmap.h	2020-10-01 12:47:19.936767240 +0200
+++ b/src/pmemd/src/cuda/kBWU_cmap.h	2020-10-01 13:06:07.039866638 +0200
@@ -293,11 +293,11 @@
     E          = ((a03*psifrac + a02)*psifrac + a01)*psifrac + a00 + phifrac*E;
     ecmap      = llrint(ENERGYSCALE * E);
   }
-  ecmap += __shfl(ecmap, tgx + 16);
-  ecmap += __shfl(ecmap, tgx +  8);
-  ecmap += __shfl(ecmap, tgx +  4);
-  ecmap += __shfl(ecmap, tgx +  2);
-  ecmap += __shfl(ecmap, tgx +  1);
+  ecmap += __shfl_sync(0xffffffff, ecmap, tgx + 16);
+  ecmap += __shfl_sync(0xffffffff, ecmap, tgx +  8);
+  ecmap += __shfl_sync(0xffffffff, ecmap, tgx +  4);
+  ecmap += __shfl_sync(0xffffffff, ecmap, tgx +  2);
+  ecmap += __shfl_sync(0xffffffff, ecmap, tgx +  1);
   if (tgx == 0) {
     nrgACC[CMAP_EACC_OFFSET + warpIdx] += ecmap;
   }
diff -Naur a/src/pmemd/src/cuda/kBWU_EnergyReduction.h b/src/pmemd/src/cuda/kBWU_EnergyReduction.h
--- a/src/pmemd/src/cuda/kBWU_EnergyReduction.h	2020-10-01 12:47:20.115767253 +0200
+++ b/src/pmemd/src/cuda/kBWU_EnergyReduction.h	2020-10-01 13:06:07.049866639 +0200
@@ -46,17 +46,17 @@
 //---------------------------------------------------------------------------------------------
 {
 #ifdef NB14_CASE
-  eel14 += __shfl(eel14, tgx + 16);
-  eel14 += __shfl(eel14, tgx +  8);
-  eel14 += __shfl(eel14, tgx +  4);
-  eel14 += __shfl(eel14, tgx +  2);
-  eel14 += __shfl(eel14, tgx +  1);
-#endif
-  eterm += __shfl(eterm, tgx + 16);
-  eterm += __shfl(eterm, tgx +  8);
-  eterm += __shfl(eterm, tgx +  4);
-  eterm += __shfl(eterm, tgx +  2);
-  eterm += __shfl(eterm, tgx +  1);
+  eel14 += __shfl_sync(0xffffffff, eel14, tgx + 16);
+  eel14 += __shfl_sync(0xffffffff, eel14, tgx +  8);
+  eel14 += __shfl_sync(0xffffffff, eel14, tgx +  4);
+  eel14 += __shfl_sync(0xffffffff, eel14, tgx +  2);
+  eel14 += __shfl_sync(0xffffffff, eel14, tgx +  1);
+#endif
+  eterm += __shfl_sync(0xffffffff, eterm, tgx + 16);
+  eterm += __shfl_sync(0xffffffff, eterm, tgx +  8);
+  eterm += __shfl_sync(0xffffffff, eterm, tgx +  4);
+  eterm += __shfl_sync(0xffffffff, eterm, tgx +  2);
+  eterm += __shfl_sync(0xffffffff, eterm, tgx +  1);
   if (tgx == 0) {
 #ifdef NB14_CASE
     nrgACC[SCEE_EACC_OFFSET + warpIdx] += eel14;
@@ -66,34 +66,34 @@
 #ifdef LOCAL_AFE
   unsigned int TIballot = __ballot(TIregion);
   if (TIballot) {
-    edvdl += __shfl(edvdl, tgx + 16);
-    edvdl += __shfl(edvdl, tgx +  8);
-    edvdl += __shfl(edvdl, tgx +  4);
-    edvdl += __shfl(edvdl, tgx +  2);
-    edvdl += __shfl(edvdl, tgx +  1);
+    edvdl += __shfl_sync(0xffffffff, edvdl, tgx + 16);
+    edvdl += __shfl_sync(0xffffffff, edvdl, tgx +  8);
+    edvdl += __shfl_sync(0xffffffff, edvdl, tgx +  4);
+    edvdl += __shfl_sync(0xffffffff, edvdl, tgx +  2);
+    edvdl += __shfl_sync(0xffffffff, edvdl, tgx +  1);
 #ifdef NB14_CASE
-    scEel14comp[0] += __shfl(scEel14comp[0], tgx + 16);
-    scEel14comp[0] += __shfl(scEel14comp[0], tgx +  8);
-    scEel14comp[0] += __shfl(scEel14comp[0], tgx +  4);
-    scEel14comp[0] += __shfl(scEel14comp[0], tgx +  2);
-    scEel14comp[0] += __shfl(scEel14comp[0], tgx +  1);
-    scEel14comp[1] += __shfl(scEel14comp[1], tgx + 16);
-    scEel14comp[1] += __shfl(scEel14comp[1], tgx +  8);
-    scEel14comp[1] += __shfl(scEel14comp[1], tgx +  4);
-    scEel14comp[1] += __shfl(scEel14comp[1], tgx +  2);
-    scEel14comp[1] += __shfl(scEel14comp[1], tgx +  1);
+    scEel14comp[0] += __shfl_sync(0xffffffff, scEel14comp[0], tgx + 16);
+    scEel14comp[0] += __shfl_sync(0xffffffff, scEel14comp[0], tgx +  8);
+    scEel14comp[0] += __shfl_sync(0xffffffff, scEel14comp[0], tgx +  4);
+    scEel14comp[0] += __shfl_sync(0xffffffff, scEel14comp[0], tgx +  2);
+    scEel14comp[0] += __shfl_sync(0xffffffff, scEel14comp[0], tgx +  1);
+    scEel14comp[1] += __shfl_sync(0xffffffff, scEel14comp[1], tgx + 16);
+    scEel14comp[1] += __shfl_sync(0xffffffff, scEel14comp[1], tgx +  8);
+    scEel14comp[1] += __shfl_sync(0xffffffff, scEel14comp[1], tgx +  4);
+    scEel14comp[1] += __shfl_sync(0xffffffff, scEel14comp[1], tgx +  2);
+    scEel14comp[1] += __shfl_sync(0xffffffff, scEel14comp[1], tgx +  1);
 #endif
 #ifndef CNST_CASE
-    scEcomp[0] += __shfl(scEcomp[0], tgx + 16);
-    scEcomp[0] += __shfl(scEcomp[0], tgx +  8);
-    scEcomp[0] += __shfl(scEcomp[0], tgx +  4);
-    scEcomp[0] += __shfl(scEcomp[0], tgx +  2);
-    scEcomp[0] += __shfl(scEcomp[0], tgx +  1);
-    scEcomp[1] += __shfl(scEcomp[1], tgx + 16);
-    scEcomp[1] += __shfl(scEcomp[1], tgx +  8);
-    scEcomp[1] += __shfl(scEcomp[1], tgx +  4);
-    scEcomp[1] += __shfl(scEcomp[1], tgx +  2);
-    scEcomp[1] += __shfl(scEcomp[1], tgx +  1);
+    scEcomp[0] += __shfl_sync(0xffffffff, scEcomp[0], tgx + 16);
+    scEcomp[0] += __shfl_sync(0xffffffff, scEcomp[0], tgx +  8);
+    scEcomp[0] += __shfl_sync(0xffffffff, scEcomp[0], tgx +  4);
+    scEcomp[0] += __shfl_sync(0xffffffff, scEcomp[0], tgx +  2);
+    scEcomp[0] += __shfl_sync(0xffffffff, scEcomp[0], tgx +  1);
+    scEcomp[1] += __shfl_sync(0xffffffff, scEcomp[1], tgx + 16);
+    scEcomp[1] += __shfl_sync(0xffffffff, scEcomp[1], tgx +  8);
+    scEcomp[1] += __shfl_sync(0xffffffff, scEcomp[1], tgx +  4);
+    scEcomp[1] += __shfl_sync(0xffffffff, scEcomp[1], tgx +  2);
+    scEcomp[1] += __shfl_sync(0xffffffff, scEcomp[1], tgx +  1);
 #endif
     if (tgx == 0) {
 
@@ -129,10 +129,10 @@
         // The warp will not diverge at this conditional over CVreport, nor
         // the one above over TIballot.  It is safe to use __shfl intrinsics.
         if (CVreport & 0x1) {
-          int barLambdaOffset = (__shfl(TIregion, i) - 1) * cSim.bar_stride;
-          bar_contrib += llrint(ENERGYSCALE * __shfl(mbarTerm, i) *
+          int barLambdaOffset = (__shfl_sync(0xffffffff, TIregion, i) - 1) * cSim.bar_stride;
+          bar_contrib += llrint(ENERGYSCALE * __shfl_sync(0xffffffff, mbarTerm, i) *
                                 (cSim.pBarLambda[barLambdaOffset + j] -
-                                 __shfl(mbarRefLambda, i)));
+                                 __shfl_sync(0xffffffff, mbarRefLambda, i)));
         }
         CVreport >>= 1;
       }
diff -Naur a/src/pmemd/src/cuda/kBWU_nb14.h b/src/pmemd/src/cuda/kBWU_nb14.h
--- a/src/pmemd/src/cuda/kBWU_nb14.h	2020-10-01 12:47:20.177767258 +0200
+++ b/src/pmemd/src/cuda/kBWU_nb14.h	2020-10-01 13:06:07.060866640 +0200
@@ -134,21 +134,21 @@
   }
 #endif // LOCAL_ENERGY
 #ifdef LOCAL_VIRIAL
-  v11 += __shfl(v11, tgx + 16);
-  v11 += __shfl(v11, tgx +  8);
-  v11 += __shfl(v11, tgx +  4);
-  v11 += __shfl(v11, tgx +  2);
-  v11 += __shfl(v11, tgx +  1);
-  v22 += __shfl(v22, tgx + 16);
-  v22 += __shfl(v22, tgx +  8);
-  v22 += __shfl(v22, tgx +  4);
-  v22 += __shfl(v22, tgx +  2);
-  v22 += __shfl(v22, tgx +  1);
-  v33 += __shfl(v33, tgx + 16);
-  v33 += __shfl(v33, tgx +  8);
-  v33 += __shfl(v33, tgx +  4);
-  v33 += __shfl(v33, tgx +  2);
-  v33 += __shfl(v33, tgx +  1);
+  v11 += __shfl_sync(0xffffffff, v11, tgx + 16);
+  v11 += __shfl_sync(0xffffffff, v11, tgx +  8);
+  v11 += __shfl_sync(0xffffffff, v11, tgx +  4);
+  v11 += __shfl_sync(0xffffffff, v11, tgx +  2);
+  v11 += __shfl_sync(0xffffffff, v11, tgx +  1);
+  v22 += __shfl_sync(0xffffffff, v22, tgx + 16);
+  v22 += __shfl_sync(0xffffffff, v22, tgx +  8);
+  v22 += __shfl_sync(0xffffffff, v22, tgx +  4);
+  v22 += __shfl_sync(0xffffffff, v22, tgx +  2);
+  v22 += __shfl_sync(0xffffffff, v22, tgx +  1);
+  v33 += __shfl_sync(0xffffffff, v33, tgx + 16);
+  v33 += __shfl_sync(0xffffffff, v33, tgx +  8);
+  v33 += __shfl_sync(0xffffffff, v33, tgx +  4);
+  v33 += __shfl_sync(0xffffffff, v33, tgx +  2);
+  v33 += __shfl_sync(0xffffffff, v33, tgx +  1);
   if (tgx == 0) {
     atomicAdd(cSim.pVirial_11, llitoulli(v11));
     atomicAdd(cSim.pVirial_22, llitoulli(v22));
diff -Naur a/src/pmemd/src/cuda/kBWU_urey.h b/src/pmemd/src/cuda/kBWU_urey.h
--- a/src/pmemd/src/cuda/kBWU_urey.h	2020-10-01 12:47:20.113767253 +0200
+++ b/src/pmemd/src/cuda/kBWU_urey.h	2020-10-01 13:06:07.073866641 +0200
@@ -67,11 +67,11 @@
 #ifdef LOCAL_ENERGY
     eurey = llrint(ENERGYSCALE * df * da);
   }
-  eurey += __shfl(eurey, tgx + 16);
-  eurey += __shfl(eurey, tgx +  8);
-  eurey += __shfl(eurey, tgx +  4);
-  eurey += __shfl(eurey, tgx +  2);
-  eurey += __shfl(eurey, tgx +  1);
+  eurey += __shfl_sync(0xffffffff, eurey, tgx + 16);
+  eurey += __shfl_sync(0xffffffff, eurey, tgx +  8);
+  eurey += __shfl_sync(0xffffffff, eurey, tgx +  4);
+  eurey += __shfl_sync(0xffffffff, eurey, tgx +  2);
+  eurey += __shfl_sync(0xffffffff, eurey, tgx +  1);
   if (tgx == 0) {
     nrgACC[UREY_EACC_OFFSET + warpIdx] += eurey;
   }
diff -Naur a/src/pmemd/src/cuda/kCalculateGBBornRadii.h b/src/pmemd/src/cuda/kCalculateGBBornRadii.h
--- a/src/pmemd/src/cuda/kCalculateGBBornRadii.h	2020-10-01 12:47:20.230767262 +0200
+++ b/src/pmemd/src/cuda/kCalculateGBBornRadii.h	2020-10-01 13:06:07.087866643 +0200
@@ -87,13 +87,13 @@
       PSATOMS(tgx) = si;
       PSATOMS2(tgx) = si2;
       PSATOMR1I(tgx) = ri1i;
-      shAtom.x   = __shfl(shAtom.x, shIdx);
-      shAtom.y   = __shfl(shAtom.y, shIdx);
-      shAtom.z   = __shfl(shAtom.z, shIdx);
-      shAtom.r   = __shfl(shAtom.r, shIdx);
-      shAtom.s   = __shfl(shAtom.s, shIdx);
-      shAtom.s2  = __shfl(shAtom.s2, shIdx);
-      shAtom.r1i = __shfl(shAtom.r1i, shIdx);
+      shAtom.x   = __shfl_sync(0xffffffff, shAtom.x, shIdx);
+      shAtom.y   = __shfl_sync(0xffffffff, shAtom.y, shIdx);
+      shAtom.z   = __shfl_sync(0xffffffff, shAtom.z, shIdx);
+      shAtom.r   = __shfl_sync(0xffffffff, shAtom.r, shIdx);
+      shAtom.s   = __shfl_sync(0xffffffff, shAtom.s, shIdx);
+      shAtom.s2  = __shfl_sync(0xffffffff, shAtom.s2, shIdx);
+      shAtom.r1i = __shfl_sync(0xffffffff, shAtom.r1i, shIdx);
       for (unsigned int j = sNext[tgx]; j != tgx; j = sNext[j]) {
         PMEFloat xij = xi - PSATOMX(j);
         PMEFloat yij = yi - PSATOMY(j);
@@ -154,13 +154,13 @@
           reff_i -= (PMEForce)dr;
 #endif
         }
-        shAtom.x   = __shfl(shAtom.x, shIdx);
-        shAtom.y   = __shfl(shAtom.y, shIdx);
-        shAtom.z   = __shfl(shAtom.z, shIdx);
-        shAtom.r   = __shfl(shAtom.r, shIdx);
-        shAtom.s   = __shfl(shAtom.s, shIdx);
-        shAtom.s2  = __shfl(shAtom.s2, shIdx);
-        shAtom.r1i = __shfl(shAtom.r1i, shIdx);
+        shAtom.x   = __shfl_sync(0xffffffff, shAtom.x, shIdx);
+        shAtom.y   = __shfl_sync(0xffffffff, shAtom.y, shIdx);
+        shAtom.z   = __shfl_sync(0xffffffff, shAtom.z, shIdx);
+        shAtom.r   = __shfl_sync(0xffffffff, shAtom.r, shIdx);
+        shAtom.s   = __shfl_sync(0xffffffff, shAtom.s, shIdx);
+        shAtom.s2  = __shfl_sync(0xffffffff, shAtom.s2, shIdx);
+        shAtom.r1i = __shfl_sync(0xffffffff, shAtom.r1i, shIdx);
       }
       int offset = x + tgx;
 #ifdef use_SPFP
@@ -305,13 +305,13 @@
           psReff[j] -= (PMEForce)dr;
 #endif // End case switch for different precision modes
         }
-        shAtom.x   = __shfl(shAtom.x, shIdx);
-        shAtom.y   = __shfl(shAtom.y, shIdx);
-        shAtom.z   = __shfl(shAtom.z, shIdx);
-        shAtom.r   = __shfl(shAtom.r, shIdx);
-        shAtom.s   = __shfl(shAtom.s, shIdx);
-        shAtom.s2  = __shfl(shAtom.s2, shIdx);
-        shAtom.r1i = __shfl(shAtom.r1i, shIdx);
+        shAtom.x   = __shfl_sync(0xffffffff, shAtom.x, shIdx);
+        shAtom.y   = __shfl_sync(0xffffffff, shAtom.y, shIdx);
+        shAtom.z   = __shfl_sync(0xffffffff, shAtom.z, shIdx);
+        shAtom.r   = __shfl_sync(0xffffffff, shAtom.r, shIdx);
+        shAtom.s   = __shfl_sync(0xffffffff, shAtom.s, shIdx);
+        shAtom.s2  = __shfl_sync(0xffffffff, shAtom.s2, shIdx);
+        shAtom.r1i = __shfl_sync(0xffffffff, shAtom.r1i, shIdx);
         j = sNext[j];
       } while (j != tgx);
       // End do ... while loop beginning roughly 130 lines ago
diff -Naur a/src/pmemd/src/cuda/kCalculateGBNonbondEnergy1.h b/src/pmemd/src/cuda/kCalculateGBNonbondEnergy1.h
--- a/src/pmemd/src/cuda/kCalculateGBNonbondEnergy1.h	2020-10-01 12:47:20.227767261 +0200
+++ b/src/pmemd/src/cuda/kCalculateGBNonbondEnergy1.h	2020-10-01 13:06:07.097866643 +0200
@@ -110,12 +110,12 @@
       PSATOMR(tgx)       = ri;
 #endif
 
-      shAtom.x    = __shfl(shAtom.x, shIdx);
-      shAtom.y    = __shfl(shAtom.y, shIdx);
-      shAtom.z    = __shfl(shAtom.z, shIdx);
-      shAtom.q    = __shfl(shAtom.q, shIdx);
-      shAtom.r    = __shfl(shAtom.r, shIdx);
-      shAtom.LJID = __shfl(shAtom.LJID, shIdx);
+      shAtom.x    = __shfl_sync(0xffffffff, shAtom.x, shIdx);
+      shAtom.y    = __shfl_sync(0xffffffff, shAtom.y, shIdx);
+      shAtom.z    = __shfl_sync(0xffffffff, shAtom.z, shIdx);
+      shAtom.q    = __shfl_sync(0xffffffff, shAtom.q, shIdx);
+      shAtom.r    = __shfl_sync(0xffffffff, shAtom.r, shIdx);
+      shAtom.LJID = __shfl_sync(0xffffffff, shAtom.LJID, shIdx);
       for (unsigned int j = sNext[tgx]; j != tgx; j = sNext[j]) {
         PMEFloat xij  = xi - PSATOMX(j);
         PMEFloat yij  = yi - PSATOMY(j);
@@ -250,12 +250,12 @@
         fz_i += (PMEDouble)dedz;
 #endif // End of pre-processor branch over different precision modes
         excl >>= 1;
-        shAtom.x    = __shfl(shAtom.x, shIdx);
-        shAtom.y    = __shfl(shAtom.y, shIdx);
-        shAtom.z    = __shfl(shAtom.z, shIdx);
-        shAtom.q    = __shfl(shAtom.q, shIdx);
-        shAtom.r    = __shfl(shAtom.r, shIdx);
-        shAtom.LJID = __shfl(shAtom.LJID, shIdx);
+        shAtom.x    = __shfl_sync(0xffffffff, shAtom.x, shIdx);
+        shAtom.y    = __shfl_sync(0xffffffff, shAtom.y, shIdx);
+        shAtom.z    = __shfl_sync(0xffffffff, shAtom.z, shIdx);
+        shAtom.q    = __shfl_sync(0xffffffff, shAtom.q, shIdx);
+        shAtom.r    = __shfl_sync(0xffffffff, shAtom.r, shIdx);
+        shAtom.LJID = __shfl_sync(0xffffffff, shAtom.LJID, shIdx);
       }
       int offset = x + tgx;
 #ifdef use_SPFP
@@ -486,18 +486,18 @@
         PSFZ(j) -= dedz;
 #endif // End of pre-processor branch over precision modes
         excl >>= 1;
-        shAtom.x    = __shfl(shAtom.x, shIdx);
-        shAtom.y    = __shfl(shAtom.y, shIdx);
-        shAtom.z    = __shfl(shAtom.z, shIdx);
-        shAtom.q    = __shfl(shAtom.q, shIdx);
-        shAtom.r    = __shfl(shAtom.r, shIdx);
-        shAtom.LJID = __shfl(shAtom.LJID, shIdx);
+        shAtom.x    = __shfl_sync(0xffffffff, shAtom.x, shIdx);
+        shAtom.y    = __shfl_sync(0xffffffff, shAtom.y, shIdx);
+        shAtom.z    = __shfl_sync(0xffffffff, shAtom.z, shIdx);
+        shAtom.q    = __shfl_sync(0xffffffff, shAtom.q, shIdx);
+        shAtom.r    = __shfl_sync(0xffffffff, shAtom.r, shIdx);
+        shAtom.LJID = __shfl_sync(0xffffffff, shAtom.LJID, shIdx);
 #ifndef GB_IGB6
   #ifdef use_SPXP
-        sumdeijda_j.x = __shfl(sumdeijda_j.x, shIdx);
-        sumdeijda_j.y = __shfl(sumdeijda_j.y, shIdx);
+        sumdeijda_j.x = __shfl_sync(0xffffffff, sumdeijda_j.x, shIdx);
+        sumdeijda_j.y = __shfl_sync(0xffffffff, sumdeijda_j.y, shIdx);
   #else
-        sumdeijda_j   = __shfl(sumdeijda_j, shIdx);
+        sumdeijda_j   = __shfl_sync(0xffffffff, sumdeijda_j, shIdx);
   #endif
 #endif //GB_IGB6
         j = sNext[j];
@@ -584,21 +584,21 @@
 
 #ifdef GB_ENERGY
   // Reduce and write energies
-  egb  += __shfl(egb, threadIdx.x ^ 1);
-  evdw += __shfl(evdw, threadIdx.x ^ 1);
-  eelt += __shfl(eelt, threadIdx.x ^ 1);
-  egb  += __shfl(egb, threadIdx.x ^ 2);
-  evdw += __shfl(evdw, threadIdx.x ^ 2);
-  eelt += __shfl(eelt, threadIdx.x ^ 2);
-  egb  += __shfl(egb, threadIdx.x ^ 4);
-  evdw += __shfl(evdw, threadIdx.x ^ 4);
-  eelt += __shfl(eelt, threadIdx.x ^ 4);
-  egb  += __shfl(egb, threadIdx.x ^ 8);
-  evdw += __shfl(evdw, threadIdx.x ^ 8);
-  eelt += __shfl(eelt, threadIdx.x ^ 8);
-  egb  += __shfl(egb, threadIdx.x ^ 16);
-  evdw += __shfl(evdw, threadIdx.x ^ 16);
-  eelt += __shfl(eelt, threadIdx.x ^ 16);
+  egb  += __shfl_sync(0xffffffff, egb, threadIdx.x ^ 1);
+  evdw += __shfl_sync(0xffffffff, evdw, threadIdx.x ^ 1);
+  eelt += __shfl_sync(0xffffffff, eelt, threadIdx.x ^ 1);
+  egb  += __shfl_sync(0xffffffff, egb, threadIdx.x ^ 2);
+  evdw += __shfl_sync(0xffffffff, evdw, threadIdx.x ^ 2);
+  eelt += __shfl_sync(0xffffffff, eelt, threadIdx.x ^ 2);
+  egb  += __shfl_sync(0xffffffff, egb, threadIdx.x ^ 4);
+  evdw += __shfl_sync(0xffffffff, evdw, threadIdx.x ^ 4);
+  eelt += __shfl_sync(0xffffffff, eelt, threadIdx.x ^ 4);
+  egb  += __shfl_sync(0xffffffff, egb, threadIdx.x ^ 8);
+  evdw += __shfl_sync(0xffffffff, evdw, threadIdx.x ^ 8);
+  eelt += __shfl_sync(0xffffffff, eelt, threadIdx.x ^ 8);
+  egb  += __shfl_sync(0xffffffff, egb, threadIdx.x ^ 16);
+  evdw += __shfl_sync(0xffffffff, evdw, threadIdx.x ^ 16);
+  eelt += __shfl_sync(0xffffffff, eelt, threadIdx.x ^ 16);
 
   // Write out energies
   if ((threadIdx.x & GRID_BITS_MASK) == 0) {
diff -Naur a/src/pmemd/src/cuda/kCalculateGBNonbondEnergy2.h b/src/pmemd/src/cuda/kCalculateGBNonbondEnergy2.h
--- a/src/pmemd/src/cuda/kCalculateGBNonbondEnergy2.h	2020-10-01 12:47:20.157767256 +0200
+++ b/src/pmemd/src/cuda/kCalculateGBNonbondEnergy2.h	2020-10-01 13:06:07.106866644 +0200
@@ -99,12 +99,12 @@
       PSATOMR(tgx)    = ri;
       PSATOMR1I(tgx)  = ri1i;
       PSATOMS(tgx)    = si;
-      shAtom.x   = __shfl(shAtom.x, shIdx);
-      shAtom.y   = __shfl(shAtom.y, shIdx);
-      shAtom.z   = __shfl(shAtom.z, shIdx);
-      shAtom.r   = __shfl(shAtom.r, shIdx);
-      shAtom.r1i = __shfl(shAtom.r1i, shIdx);
-      shAtom.s   = __shfl(shAtom.s, shIdx);
+      shAtom.x   = __shfl_sync(0xffffffff, shAtom.x, shIdx);
+      shAtom.y   = __shfl_sync(0xffffffff, shAtom.y, shIdx);
+      shAtom.z   = __shfl_sync(0xffffffff, shAtom.z, shIdx);
+      shAtom.r   = __shfl_sync(0xffffffff, shAtom.r, shIdx);
+      shAtom.r1i = __shfl_sync(0xffffffff, shAtom.r1i, shIdx);
+      shAtom.s   = __shfl_sync(0xffffffff, shAtom.s, shIdx);
       for (unsigned int j = sNext[tgx]; j != tgx; j = sNext[j]) {
         PMEFloat xij = xi - PSATOMX(j);
         PMEFloat yij = yi - PSATOMY(j);
@@ -202,12 +202,12 @@
           fz_i         -= f_z;
 #endif // End pre-processor branch over precision modes
         }
-        shAtom.x   = __shfl(shAtom.x, shIdx);
-        shAtom.y   = __shfl(shAtom.y, shIdx);
-        shAtom.z   = __shfl(shAtom.z, shIdx);
-        shAtom.r   = __shfl(shAtom.r, shIdx);
-        shAtom.r1i = __shfl(shAtom.r1i, shIdx);
-        shAtom.s   = __shfl(shAtom.s, shIdx);
+        shAtom.x   = __shfl_sync(0xffffffff, shAtom.x, shIdx);
+        shAtom.y   = __shfl_sync(0xffffffff, shAtom.y, shIdx);
+        shAtom.z   = __shfl_sync(0xffffffff, shAtom.z, shIdx);
+        shAtom.r   = __shfl_sync(0xffffffff, shAtom.r, shIdx);
+        shAtom.r1i = __shfl_sync(0xffffffff, shAtom.r1i, shIdx);
+        shAtom.s   = __shfl_sync(0xffffffff, shAtom.s, shIdx);
       }
       // Here ends a loop over j that has the non-bonded calculation hopping around
       // in a list of interactions
@@ -459,13 +459,13 @@
           fz_i         += f_z;
 #endif
         }
-        shAtom.x     = __shfl(shAtom.x, shIdx);
-        shAtom.y     = __shfl(shAtom.y, shIdx);
-        shAtom.z     = __shfl(shAtom.z, shIdx);
-        shAtom.r     = __shfl(shAtom.r, shIdx);
-        shAtom.r1i   = __shfl(shAtom.r1i, shIdx);
-        shAtom.s     = __shfl(shAtom.s, shIdx);
-        shAtom.temp7 = __shfl(shAtom.temp7, shIdx);
+        shAtom.x     = __shfl_sync(0xffffffff, shAtom.x, shIdx);
+        shAtom.y     = __shfl_sync(0xffffffff, shAtom.y, shIdx);
+        shAtom.z     = __shfl_sync(0xffffffff, shAtom.z, shIdx);
+        shAtom.r     = __shfl_sync(0xffffffff, shAtom.r, shIdx);
+        shAtom.r1i   = __shfl_sync(0xffffffff, shAtom.r1i, shIdx);
+        shAtom.s     = __shfl_sync(0xffffffff, shAtom.s, shIdx);
+        shAtom.temp7 = __shfl_sync(0xffffffff, shAtom.temp7, shIdx);
         j = sNext[j];
       } while (j != tgx);
 
diff -Naur a/src/pmemd/src/cuda/kCalculateNEBForces.cu b/src/pmemd/src/cuda/kCalculateNEBForces.cu
--- a/src/pmemd/src/cuda/kCalculateNEBForces.cu	2020-10-01 12:47:20.417767276 +0200
+++ b/src/pmemd/src/cuda/kCalculateNEBForces.cu	2020-10-01 13:06:07.116866645 +0200
@@ -53,7 +53,7 @@
 //----------------------------------------------------------------------------------------------
 __inline__ __device__ double warpReduceSum(double val) {
   for (int offset = GRID/2; offset > 0; offset /= 2) 
-    val += __shfl_down(val, offset);
+    val += __shfl_down_sync(0xffffffff, val, offset);
   return val;
 }
 
diff -Naur a/src/pmemd/src/cuda/kCalculatePMENonbondEnergy.cu b/src/pmemd/src/cuda/kCalculatePMENonbondEnergy.cu
--- a/src/pmemd/src/cuda/kCalculatePMENonbondEnergy.cu	2020-10-01 12:47:20.120767253 +0200
+++ b/src/pmemd/src/cuda/kCalculatePMENonbondEnergy.cu	2020-10-01 13:06:07.130866647 +0200
@@ -168,21 +168,21 @@
 	double remain = target - (((double)pcoef.x * r2) + ((double)pcoef.z / r2) +
 				  ((double)pcoef.w / (r2 * r2)));
 	double pcy = remain;
-        pcy += __shfl_xor(pcy, 16);
-	pcy += __shfl_xor(pcy,  8);
-	pcy += __shfl_xor(pcy,  4);
-	pcy += __shfl_xor(pcy,  2);
-	pcy += __shfl_xor(pcy,  1);
+        pcy += __shfl_xor_sync(0xffffffff, pcy, 16);
+	pcy += __shfl_xor_sync(0xffffffff, pcy,  8);
+	pcy += __shfl_xor_sync(0xffffffff, pcy,  4);
+	pcy += __shfl_xor_sync(0xffffffff, pcy,  2);
+	pcy += __shfl_xor_sync(0xffffffff, pcy,  1);
 	pcoef.y = (PMEFloat)(pcy * 0.03125);
 
 	// Compute the mean error
 	remain -= (double)pcoef.y;
 	remain *= remain;
-	remain += __shfl_xor(remain, 16);
-	remain += __shfl_xor(remain,  8);
-	remain += __shfl_xor(remain,  4);
-	remain += __shfl_xor(remain,  2);
-	remain += __shfl_xor(remain,  1);
+	remain += __shfl_xor_sync(0xffffffff, remain, 16);
+	remain += __shfl_xor_sync(0xffffffff, remain,  8);
+	remain += __shfl_xor_sync(0xffffffff, remain,  4);
+	remain += __shfl_xor_sync(0xffffffff, remain,  2);
+	remain += __shfl_xor_sync(0xffffffff, remain,  1);
 	remain *= 0.03125;
 	if (remain < minr) {
 	  minr = remain;
diff -Naur a/src/pmemd/src/cuda/kNeighborList.cu b/src/pmemd/src/cuda/kNeighborList.cu
--- a/src/pmemd/src/cuda/kNeighborList.cu	2020-10-01 12:47:20.264767264 +0200
+++ b/src/pmemd/src/cuda/kNeighborList.cu	2020-10-01 13:06:07.143866648 +0200
@@ -760,11 +760,11 @@
 
       // Group 256 consecutive cells and compute the sum of their populations
       __syncthreads();
-      cellpop += __shfl_down(cellpop, 16);
-      cellpop += __shfl_down(cellpop, 8);
-      cellpop += __shfl_down(cellpop, 4);
-      cellpop += __shfl_down(cellpop, 2);
-      cellpop += __shfl_down(cellpop, 1);
+      cellpop += __shfl_down_sync(0xffffffff, cellpop, 16);
+      cellpop += __shfl_down_sync(0xffffffff, cellpop, 8);
+      cellpop += __shfl_down_sync(0xffffffff, cellpop, 4);
+      cellpop += __shfl_down_sync(0xffffffff, cellpop, 2);
+      cellpop += __shfl_down_sync(0xffffffff, cellpop, 1);
       if ((threadIdx.x & GRID_BITS_MASK) == 0) {
         groupSums[8*blockIdx.x + (threadIdx.x >> GRID_BITS)] = cellpop;
       }
@@ -773,9 +773,9 @@
       if (threadIdx.x < 8) {
         qqgs = groupSums[8*blockIdx.x + threadIdx.x];
       }
-      qqgs += __shfl_down(qqgs, 4);
-      qqgs += __shfl_down(qqgs, 2);
-      qqgs += __shfl_down(qqgs, 1);
+      qqgs += __shfl_down_sync(0xffffffff, qqgs, 4);
+      qqgs += __shfl_down_sync(0xffffffff, qqgs, 2);
+      qqgs += __shfl_down_sync(0xffffffff, qqgs, 1);
       if (threadIdx.x == 0) {
         cSim.pHcmbQQPopHeapSums[cellidx / 256] = qqgs;
       }
@@ -813,11 +813,11 @@
 
       // Group 256 consecutive cells and compute the sum of their populations
       __syncthreads();
-      cellpop += __shfl_down(cellpop, 16);
-      cellpop += __shfl_down(cellpop, 8);
-      cellpop += __shfl_down(cellpop, 4);
-      cellpop += __shfl_down(cellpop, 2);
-      cellpop += __shfl_down(cellpop, 1);
+      cellpop += __shfl_down_sync(0xffffffff, cellpop, 16);
+      cellpop += __shfl_down_sync(0xffffffff, cellpop, 8);
+      cellpop += __shfl_down_sync(0xffffffff, cellpop, 4);
+      cellpop += __shfl_down_sync(0xffffffff, cellpop, 2);
+      cellpop += __shfl_down_sync(0xffffffff, cellpop, 1);
       if ((threadIdx.x & GRID_BITS_MASK) == 0) {
         groupSums[8*blockIdx.x + (threadIdx.x >> GRID_BITS)] = cellpop;
       }
@@ -825,9 +825,9 @@
       if (threadIdx.x < 8) {
         int ljgs = 0;
         ljgs = groupSums[8*blockIdx.x + threadIdx.x];
-        ljgs += __shfl_down(ljgs, 4);
-        ljgs += __shfl_down(ljgs, 2);
-        ljgs += __shfl_down(ljgs, 1);
+        ljgs += __shfl_down_sync(0xffffffff, ljgs, 4);
+        ljgs += __shfl_down_sync(0xffffffff, ljgs, 2);
+        ljgs += __shfl_down_sync(0xffffffff, ljgs, 1);
         if (threadIdx.x == 0) {
           cSim.pHcmbLJPopHeapSums[cellidx / 256] = ljgs;
         }
diff -Naur a/src/pmemd/src/cuda/kNLCINE.h b/src/pmemd/src/cuda/kNLCINE.h
--- a/src/pmemd/src/cuda/kNLCINE.h	2020-10-01 12:47:20.282767265 +0200
+++ b/src/pmemd/src/cuda/kNLCINE.h	2020-10-01 13:06:07.155866649 +0200
@@ -218,21 +218,21 @@
 #if (IPS_ATOMS_PER_WARP == 32)
       unsigned int j = sNext[tgx];
       unsigned int shIdx = j;
-      shAtom.x    = __shfl(shAtom.x, j);
-      shAtom.y    = __shfl(shAtom.y, j);
-      shAtom.z    = __shfl(shAtom.z, j);
-      shAtom.q    = __shfl(shAtom.q, j);
-      shAtom.LJID = __shfl(shAtom.LJID, j);
+      shAtom.x    = __shfl_sync(0xffffffff, shAtom.x, j);
+      shAtom.y    = __shfl_sync(0xffffffff, shAtom.y, j);
+      shAtom.z    = __shfl_sync(0xffffffff, shAtom.z, j);
+      shAtom.q    = __shfl_sync(0xffffffff, shAtom.q, j);
+      shAtom.LJID = __shfl_sync(0xffffffff, shAtom.LJID, j);
       while (j != tgx) {
 #else
       unsigned int j   = sStart[tgx];
       unsigned int end = sEnd[tgx];
       unsigned int shIdx = sNext[tgx];
-      shAtom.x    = __shfl(shAtom.x, j);
-      shAtom.y    = __shfl(shAtom.y, j);
-      shAtom.z    = __shfl(shAtom.z, j);
-      shAtom.q    = __shfl(shAtom.q, j);
-      shAtom.LJID = __shfl(shAtom.LJID, j);
+      shAtom.x    = __shfl_sync(0xffffffff, shAtom.x, j);
+      shAtom.y    = __shfl_sync(0xffffffff, shAtom.y, j);
+      shAtom.z    = __shfl_sync(0xffffffff, shAtom.z, j);
+      shAtom.q    = __shfl_sync(0xffffffff, shAtom.q, j);
+      shAtom.LJID = __shfl_sync(0xffffffff, shAtom.LJID, j);
       while (j != end) {
 #endif
         PMEFloat xij = xi - PSATOMX(j);
@@ -322,11 +322,11 @@
 
         // Shift the exclusion counter for the next atom
         exclusion >>= 1;
-        shAtom.x    = __shfl(shAtom.x, shIdx);
-        shAtom.y    = __shfl(shAtom.y, shIdx);
-        shAtom.z    = __shfl(shAtom.z, shIdx);
-        shAtom.q    = __shfl(shAtom.q, shIdx);
-        shAtom.LJID = __shfl(shAtom.LJID, shIdx);
+        shAtom.x    = __shfl_sync(0xffffffff, shAtom.x, shIdx);
+        shAtom.y    = __shfl_sync(0xffffffff, shAtom.y, shIdx);
+        shAtom.z    = __shfl_sync(0xffffffff, shAtom.z, shIdx);
+        shAtom.q    = __shfl_sync(0xffffffff, shAtom.q, shIdx);
+        shAtom.LJID = __shfl_sync(0xffffffff, shAtom.LJID, shIdx);
         j = sNext[j];
 #if (IPS_ATOMS_PER_WARP == 32)
       }
@@ -521,11 +521,11 @@
 
           // Shift the exclusion counter to prepare for the next atom
           exclusion >>= 1;
-          shAtom.x    = __shfl(shAtom.x, shIdx);
-          shAtom.y    = __shfl(shAtom.y, shIdx);
-          shAtom.z    = __shfl(shAtom.z, shIdx);
-          shAtom.q    = __shfl(shAtom.q, shIdx);
-          shAtom.LJID = __shfl(shAtom.LJID, shIdx);
+          shAtom.x    = __shfl_sync(0xffffffff, shAtom.x, shIdx);
+          shAtom.y    = __shfl_sync(0xffffffff, shAtom.y, shIdx);
+          shAtom.z    = __shfl_sync(0xffffffff, shAtom.z, shIdx);
+          shAtom.q    = __shfl_sync(0xffffffff, shAtom.q, shIdx);
+          shAtom.LJID = __shfl_sync(0xffffffff, shAtom.LJID, shIdx);
           j = sNext[j];
         } while (j != tgx);
         // Here ends the inner loop for the case that exclusions ARE present.
@@ -617,11 +617,11 @@
   #endif
 #endif
           }
-          shAtom.x    = __shfl(shAtom.x, shIdx);
-          shAtom.y    = __shfl(shAtom.y, shIdx);
-          shAtom.z    = __shfl(shAtom.z, shIdx);
-          shAtom.q    = __shfl(shAtom.q, shIdx);
-          shAtom.LJID = __shfl(shAtom.LJID, shIdx);
+          shAtom.x    = __shfl_sync(0xffffffff, shAtom.x, shIdx);
+          shAtom.y    = __shfl_sync(0xffffffff, shAtom.y, shIdx);
+          shAtom.z    = __shfl_sync(0xffffffff, shAtom.z, shIdx);
+          shAtom.q    = __shfl_sync(0xffffffff, shAtom.q, shIdx);
+          shAtom.LJID = __shfl_sync(0xffffffff, shAtom.LJID, shIdx);
           j = sNext[j];
         } while (j != tgx);
         // Here ends the inner loop for the case that no exclusions will be encountered.
diff -Naur a/src/pmemd/src/cuda/kNLCPNE_AFE.h b/src/pmemd/src/cuda/kNLCPNE_AFE.h
--- a/src/pmemd/src/cuda/kNLCPNE_AFE.h	2020-10-01 12:47:20.065767249 +0200
+++ b/src/pmemd/src/cuda/kNLCPNE_AFE.h	2020-10-01 13:06:07.169866651 +0200
@@ -294,13 +294,13 @@
       unsigned int end = sEnd[tgx];
 #endif
       unsigned int shIdx = j;
-      shAtom.x    = __shfl(shAtom.x, j);
-      shAtom.y    = __shfl(shAtom.y, j);
-      shAtom.z    = __shfl(shAtom.z, j);
-      shAtom.q    = __shfl(shAtom.q, j);
-      shAtom.LJID = __shfl(shAtom.LJID, j);
+      shAtom.x    = __shfl_sync(0xffffffff, shAtom.x, j);
+      shAtom.y    = __shfl_sync(0xffffffff, shAtom.y, j);
+      shAtom.z    = __shfl_sync(0xffffffff, shAtom.z, j);
+      shAtom.q    = __shfl_sync(0xffffffff, shAtom.q, j);
+      shAtom.LJID = __shfl_sync(0xffffffff, shAtom.LJID, j);
 #ifdef PME_SCTI
-      shAtom.TI   = __shfl(shAtom.TI, j);
+      shAtom.TI   = __shfl_sync(0xffffffff, shAtom.TI, j);
 #endif
 
 #ifdef PME_SCTI
@@ -444,11 +444,11 @@
 #endif
           }
           exclusion >>= 1;
-          shAtom.x    = __shfl(shAtom.x, shIdx);
-          shAtom.y    = __shfl(shAtom.y, shIdx);
-          shAtom.z    = __shfl(shAtom.z, shIdx);
-          shAtom.q    = __shfl(shAtom.q, shIdx);
-          shAtom.LJID = __shfl(shAtom.LJID, shIdx);
+          shAtom.x    = __shfl_sync(0xffffffff, shAtom.x, shIdx);
+          shAtom.y    = __shfl_sync(0xffffffff, shAtom.y, shIdx);
+          shAtom.z    = __shfl_sync(0xffffffff, shAtom.z, shIdx);
+          shAtom.q    = __shfl_sync(0xffffffff, shAtom.q, shIdx);
+          shAtom.LJID = __shfl_sync(0xffffffff, shAtom.LJID, shIdx);
           j = sNext[j];
 #if (PME_ATOMS_PER_WARP == 32)
         }
@@ -927,12 +927,12 @@
             }
           }
           exclusion >>= 1;
-          shAtom.x    = __shfl(shAtom.x, shIdx);
-          shAtom.y    = __shfl(shAtom.y, shIdx);
-          shAtom.z    = __shfl(shAtom.z, shIdx);
-          shAtom.q    = __shfl(shAtom.q, shIdx);
-          shAtom.LJID = __shfl(shAtom.LJID, shIdx);
-          shAtom.TI   = __shfl(shAtom.TI, shIdx);
+          shAtom.x    = __shfl_sync(0xffffffff, shAtom.x, shIdx);
+          shAtom.y    = __shfl_sync(0xffffffff, shAtom.y, shIdx);
+          shAtom.z    = __shfl_sync(0xffffffff, shAtom.z, shIdx);
+          shAtom.q    = __shfl_sync(0xffffffff, shAtom.q, shIdx);
+          shAtom.LJID = __shfl_sync(0xffffffff, shAtom.LJID, shIdx);
+          shAtom.TI   = __shfl_sync(0xffffffff, shAtom.TI, shIdx);
           j = sNext[j];
 #if (PME_ATOMS_PER_WARP == 32)
         }
@@ -1507,12 +1507,12 @@
           // (111 or 110, see the possible three-bit outcomes way up above)
 
           exclusion >>= 1;
-          shAtom.x    = __shfl(shAtom.x, shIdx);
-          shAtom.y    = __shfl(shAtom.y, shIdx);
-          shAtom.z    = __shfl(shAtom.z, shIdx);
-          shAtom.q    = __shfl(shAtom.q, shIdx);
-          shAtom.LJID = __shfl(shAtom.LJID, shIdx);
-          shAtom.TI   = __shfl(shAtom.TI, shIdx);
+          shAtom.x    = __shfl_sync(0xffffffff, shAtom.x, shIdx);
+          shAtom.y    = __shfl_sync(0xffffffff, shAtom.y, shIdx);
+          shAtom.z    = __shfl_sync(0xffffffff, shAtom.z, shIdx);
+          shAtom.q    = __shfl_sync(0xffffffff, shAtom.q, shIdx);
+          shAtom.LJID = __shfl_sync(0xffffffff, shAtom.LJID, shIdx);
+          shAtom.TI   = __shfl_sync(0xffffffff, shAtom.TI, shIdx);
           j = sNext[j];
         } while (j != tgx);
       }
@@ -1673,11 +1673,11 @@
 #endif // End pre-processor branch over different precision modes
             }
             exclusion >>= 1;
-            shAtom.x    = __shfl(shAtom.x, shIdx);
-            shAtom.y    = __shfl(shAtom.y, shIdx);
-            shAtom.z    = __shfl(shAtom.z, shIdx);
-            shAtom.q    = __shfl(shAtom.q, shIdx);
-            shAtom.LJID = __shfl(shAtom.LJID, shIdx);
+            shAtom.x    = __shfl_sync(0xffffffff, shAtom.x, shIdx);
+            shAtom.y    = __shfl_sync(0xffffffff, shAtom.y, shIdx);
+            shAtom.z    = __shfl_sync(0xffffffff, shAtom.z, shIdx);
+            shAtom.q    = __shfl_sync(0xffffffff, shAtom.q, shIdx);
+            shAtom.LJID = __shfl_sync(0xffffffff, shAtom.LJID, shIdx);
             j = sNext[j];
           } while (j != tgx);
         }
@@ -1842,11 +1842,11 @@
   #endif
 #endif
             }
-            shAtom.x    = __shfl(shAtom.x, shIdx);
-            shAtom.y    = __shfl(shAtom.y, shIdx);
-            shAtom.z    = __shfl(shAtom.z, shIdx);
-            shAtom.q    = __shfl(shAtom.q, shIdx);
-            shAtom.LJID = __shfl(shAtom.LJID, shIdx);
+            shAtom.x    = __shfl_sync(0xffffffff, shAtom.x, shIdx);
+            shAtom.y    = __shfl_sync(0xffffffff, shAtom.y, shIdx);
+            shAtom.z    = __shfl_sync(0xffffffff, shAtom.z, shIdx);
+            shAtom.q    = __shfl_sync(0xffffffff, shAtom.q, shIdx);
+            shAtom.LJID = __shfl_sync(0xffffffff, shAtom.LJID, shIdx);
             j = sNext[j];
           } while (j != tgx);
         }
diff -Naur a/src/pmemd/src/cuda/kNLCPNE.h b/src/pmemd/src/cuda/kNLCPNE.h
--- a/src/pmemd/src/cuda/kNLCPNE.h	2020-10-01 12:47:20.049767248 +0200
+++ b/src/pmemd/src/cuda/kNLCPNE.h	2020-10-01 13:06:07.180866652 +0200
@@ -201,11 +201,11 @@
                                    (tgx & cSim.NLAtomsPerWarpBitsMask)];
       psWarp->nlEntry.NL.offset += cSim.NLAtomsPerWarp;
 #if (PME_ATOMS_PER_WARP == 16)
-      exclusion = __shfl_up(exclusion, 16);
+      exclusion = __shfl_up_sync(0xffffffff, exclusion, 16);
       exclusion >>= 4*(tgx >> cSim.NLAtomsPerWarpBits);
 #elif (PME_ATOMS_PER_WARP == 8)
-      exclusion = __shfl_up(exclusion, 16);
-      exclusion = __shfl_up(exclusion, 8);
+      exclusion = __shfl_up_sync(0xffffffff, exclusion, 16);
+      exclusion = __shfl_up_sync(0xffffffff, exclusion, 8);
       exclusion >>= (tgx >> cSim.NLAtomsPerWarpBits);
 #endif
       // In the first tile, atoms A, B, C, ... N are interacting with each other, not
@@ -213,7 +213,7 @@
       // store its j atom in addition to its i atom, is not needed in this context.
       // However, the Lennard Jones index of each i atom gets multiplied by the total
       // number of types to prepare for indexing into the table, so store the original
-      // value for future __shfl() reference.
+      // value for future __shfl_sync(0xffffffff, ) reference.
       unsigned int LJIDj = LJIDi;
       LJIDi *= cSim.LJTypes;
 
@@ -253,12 +253,12 @@
       const int iftlim = 1;
 #endif
       for (ift = 0; ift < iftlim; ift++) {
-        PMEFloat xij = xi - __shfl(xi, j);
-        PMEFloat yij = yi - __shfl(yi, j);
-        PMEFloat zij = zi - __shfl(zi, j);
+        PMEFloat xij = xi - __shfl_sync(0xffffffff, xi, j);
+        PMEFloat yij = yi - __shfl_sync(0xffffffff, yi, j);
+        PMEFloat zij = zi - __shfl_sync(0xffffffff, zi, j);
         PMEFloat r2  = xij*xij + yij*yij + zij*zij;
-        unsigned int index = LJIDi + __shfl(LJIDj, j);
-        PMEFloat qiqj = qi * __shfl(qi, j);
+        unsigned int index = LJIDi + __shfl_sync(0xffffffff, LJIDj, j);
+        PMEFloat qiqj = qi * __shfl_sync(0xffffffff, qi, j);
 	PMEFloat df = (PMEFloat)0.0;
         int inrange = ((r2 < cSim.cut2) && r2 > delta);
         if (inrange) {
@@ -411,9 +411,9 @@
         TLx_i += dfdx;
         TLy_i += dfdy;
         TLz_i += dfdz;
-        TLx_i -= __shfl(dfdx, jrec);
-        TLy_i -= __shfl(dfdy, jrec);
-        TLz_i -= __shfl(dfdz, jrec);
+        TLx_i -= __shfl_sync(0xffffffff, dfdx, jrec);
+        TLy_i -= __shfl_sync(0xffffffff, dfdy, jrec);
+        TLz_i -= __shfl_sync(0xffffffff, dfdz, jrec);
 #  ifdef PME_VIRIAL
         TLvir_11 -= xij * dfdx;
         TLvir_22 -= yij * dfdy;
@@ -423,9 +423,9 @@
         fx_i.pmef += (PMEForce)dfdx;
         fy_i.pmef += (PMEForce)dfdy;
         fz_i.pmef += (PMEForce)dfdz;
-        fx_i.pmef -= (PMEForce)(__shfl(dfdx, jrec));
-        fy_i.pmef -= (PMEForce)(__shfl(dfdy, jrec));
-        fz_i.pmef -= (PMEForce)(__shfl(dfdz, jrec));
+        fx_i.pmef -= (PMEForce)(__shfl_sync(0xffffffff, dfdx, jrec));
+        fy_i.pmef -= (PMEForce)(__shfl_sync(0xffffffff, dfdy, jrec));
+        fz_i.pmef -= (PMEForce)(__shfl_sync(0xffffffff, dfdz, jrec));
 #  ifdef PME_VIRIAL
         vir_11.pmev -= (PMEVirial)(xij * dfdx);
         vir_22.pmev -= (PMEVirial)(yij * dfdy);
@@ -571,12 +571,12 @@
       if (__any(exclusion)) {
 #pragma unroll 2
         do {
-          PMEFloat xij = xi - __shfl(shAtom.x, j);
-          PMEFloat yij = yi - __shfl(shAtom.y, j);
-          PMEFloat zij = zi - __shfl(shAtom.z, j);
+          PMEFloat xij = xi - __shfl_sync(0xffffffff, shAtom.x, j);
+          PMEFloat yij = yi - __shfl_sync(0xffffffff, shAtom.y, j);
+          PMEFloat zij = zi - __shfl_sync(0xffffffff, shAtom.z, j);
           PMEFloat r2  = xij*xij + yij*yij + zij*zij;
-          unsigned int index = LJIDi + __shfl(shAtom.LJID, j);
-          PMEFloat qiqj = qi * __shfl(shAtom.q, j);
+          unsigned int index = LJIDi + __shfl_sync(0xffffffff, shAtom.LJID, j);
+          PMEFloat qiqj = qi * __shfl_sync(0xffffffff, shAtom.q, j);
 	  PMEFloat df = (PMEFloat)0.0;
           int inrange = ((r2 < cSim.cut2) && r2 > delta);
           if (inrange) {
@@ -697,9 +697,9 @@
           TLx_i += dfdx;
           TLy_i += dfdy;
           TLz_i += dfdz;
-          shFx -= __shfl(dfdx, jrec);
-          shFy -= __shfl(dfdy, jrec);
-          shFz -= __shfl(dfdz, jrec);
+          shFx -= __shfl_sync(0xffffffff, dfdx, jrec);
+          shFy -= __shfl_sync(0xffffffff, dfdy, jrec);
+          shFz -= __shfl_sync(0xffffffff, dfdz, jrec);
 #  ifdef PME_VIRIAL
           TLvir_11 -= xij * dfdx;
           TLvir_22 -= yij * dfdy;
@@ -712,9 +712,9 @@
           fx_i.pmef += dfdx1;
           fy_i.pmef += dfdy1;
           fz_i.pmef += dfdz1;
-          shFx -= __shfl(dfdx1, jrec);
-          shFy -= __shfl(dfdy1, jrec);
-          shFz -= __shfl(dfdz1, jrec);
+          shFx -= __shfl_sync(0xffffffff, dfdx1, jrec);
+          shFy -= __shfl_sync(0xffffffff, dfdy1, jrec);
+          shFz -= __shfl_sync(0xffffffff, dfdz1, jrec);
 #  ifdef PME_VIRIAL
           vir_11.pmev -= (PMEForce)(xij * dfdx);
           vir_22.pmev -= (PMEForce)(yij * dfdy);
@@ -733,14 +733,14 @@
         do {
 
           // Read properties for the other atom
-          PMEFloat xij = xi - __shfl(shAtom.x, j);
-          PMEFloat yij = yi - __shfl(shAtom.y, j);
-          PMEFloat zij = zi - __shfl(shAtom.z, j);
+          PMEFloat xij = xi - __shfl_sync(0xffffffff, shAtom.x, j);
+          PMEFloat yij = yi - __shfl_sync(0xffffffff, shAtom.y, j);
+          PMEFloat zij = zi - __shfl_sync(0xffffffff, shAtom.z, j);
 
           // Perform the range test
           PMEFloat r2  = xij*xij + yij*yij + zij*zij;
-          unsigned int index = LJIDi + __shfl(shAtom.LJID, j);
-          PMEFloat qiqj = qi * __shfl(shAtom.q, j);
+          unsigned int index = LJIDi + __shfl_sync(0xffffffff, shAtom.LJID, j);
+          PMEFloat qiqj = qi * __shfl_sync(0xffffffff, shAtom.q, j);
           int inrange = ((r2 < cSim.cut2) && r2 > delta);
           PMEFloat df = (PMEFloat)0.0;
           if (inrange) {
@@ -860,9 +860,9 @@
           TLx_i += dfdx;
           TLy_i += dfdy;
           TLz_i += dfdz;
-          shFx -= __shfl(dfdx, jrec);
-          shFy -= __shfl(dfdy, jrec);
-          shFz -= __shfl(dfdz, jrec);
+          shFx -= __shfl_sync(0xffffffff, dfdx, jrec);
+          shFy -= __shfl_sync(0xffffffff, dfdy, jrec);
+          shFz -= __shfl_sync(0xffffffff, dfdz, jrec);
 #  ifdef PME_VIRIAL
           TLvir_11 -= xij * dfdx;
           TLvir_22 -= yij * dfdy;
@@ -875,9 +875,9 @@
           fx_i.pmef += dfdx1;
           fy_i.pmef += dfdy1;
           fz_i.pmef += dfdz1;
-          shFx -= __shfl(dfdx1, jrec);
-          shFy -= __shfl(dfdy1, jrec);
-          shFz -= __shfl(dfdz1, jrec);
+          shFx -= __shfl_sync(0xffffffff, dfdx1, jrec);
+          shFy -= __shfl_sync(0xffffffff, dfdy1, jrec);
+          shFz -= __shfl_sync(0xffffffff, dfdz1, jrec);
 #  ifdef PME_VIRIAL
           vir_11.pmev -= (PMEForce)(xij * dfdx);
           vir_22.pmev -= (PMEForce)(yij * dfdy);
@@ -946,22 +946,22 @@
     // reduction is always necessary.  An additional 8-stride reduction may be necessary.
 #if (PME_ATOMS_PER_WARP <= 16)
 #  ifdef use_SPFP
-    fx_i.lli += __shfl(fx_i.lli, tgx + 16);
-    fy_i.lli += __shfl(fy_i.lli, tgx + 16);
-    fz_i.lli += __shfl(fz_i.lli, tgx + 16);
+    fx_i.lli += __shfl_sync(0xffffffff, fx_i.lli, tgx + 16);
+    fy_i.lli += __shfl_sync(0xffffffff, fy_i.lli, tgx + 16);
+    fz_i.lli += __shfl_sync(0xffffffff, fz_i.lli, tgx + 16);
 #    if (PME_ATOMS_PER_WARP == 8)
-    fx_i.lli += __shfl(fx_i.lli, tgx + 8);
-    fy_i.lli += __shfl(fy_i.lli, tgx + 8);
-    fz_i.lli += __shfl(fz_i.lli, tgx + 8);
+    fx_i.lli += __shfl_sync(0xffffffff, fx_i.lli, tgx + 8);
+    fy_i.lli += __shfl_sync(0xffffffff, fy_i.lli, tgx + 8);
+    fz_i.lli += __shfl_sync(0xffffffff, fz_i.lli, tgx + 8);
 #    endif
 #  else  // use_DPFP
-    fx_i.pmef += __shfl(fx_i.pmef, tgx + 16);
-    fy_i.pmef += __shfl(fy_i.pmef, tgx + 16);
-    fz_i.pmef += __shfl(fz_i.pmef, tgx + 16);
+    fx_i.pmef += __shfl_sync(0xffffffff, fx_i.pmef, tgx + 16);
+    fy_i.pmef += __shfl_sync(0xffffffff, fy_i.pmef, tgx + 16);
+    fz_i.pmef += __shfl_sync(0xffffffff, fz_i.pmef, tgx + 16);
 #    if (PME_ATOMS_PER_WARP == 8)
-    fx_i.pmef += __shfl(fx_i.pmef, tgx + 8);
-    fy_i.pmef += __shfl(fy_i.pmef, tgx + 8);
-    fz_i.pmef += __shfl(fz_i.pmef, tgx + 8);
+    fx_i.pmef += __shfl_sync(0xffffffff, fx_i.pmef, tgx + 8);
+    fy_i.pmef += __shfl_sync(0xffffffff, fy_i.pmef, tgx + 8);
+    fz_i.pmef += __shfl_sync(0xffffffff, fz_i.pmef, tgx + 8);
 #    endif
 #  endif
 #endif // End pre-processor branch for accumulating register
@@ -998,13 +998,13 @@
     unsigned int rlev;
     for (rlev = 16; rlev > 0; rlev /= 2) {
 #  ifdef use_SPFP
-      vir_11.lli += __shfl(vir_11.lli, tgx + rlev);
-      vir_22.lli += __shfl(vir_22.lli, tgx + rlev);
-      vir_33.lli += __shfl(vir_33.lli, tgx + rlev);
+      vir_11.lli += __shfl_sync(0xffffffff, vir_11.lli, tgx + rlev);
+      vir_22.lli += __shfl_sync(0xffffffff, vir_22.lli, tgx + rlev);
+      vir_33.lli += __shfl_sync(0xffffffff, vir_33.lli, tgx + rlev);
 #  else  // use_DPFP
-      vir_11.pmef += __shfl(vir_11.pmef, tgx + rlev);
-      vir_22.pmef += __shfl(vir_22.pmef, tgx + rlev);
-      vir_33.pmef += __shfl(vir_33.pmef, tgx + rlev);
+      vir_11.pmef += __shfl_sync(0xffffffff, vir_11.pmef, tgx + rlev);
+      vir_22.pmef += __shfl_sync(0xffffffff, vir_22.pmef, tgx + rlev);
+      vir_33.pmef += __shfl_sync(0xffffffff, vir_33.pmef, tgx + rlev);
 #  endif
     }
     volatile NLVirial* psV = &sWarpVirial[threadIdx.x / GRID];
@@ -1044,11 +1044,11 @@
   unsigned int rlev;
   for (rlev = 16; rlev > 0; rlev /= 2) {
 #  ifndef use_DPFP
-    eed.lli  += __shfl(eed.lli, tgx + rlev);
-    evdw.lli += __shfl(evdw.lli, tgx + rlev);
+    eed.lli  += __shfl_sync(0xffffffff, eed.lli, tgx + rlev);
+    evdw.lli += __shfl_sync(0xffffffff, evdw.lli, tgx + rlev);
 #  else
-    eed.pmef  += __shfl(eed.pmef, tgx + rlev);
-    evdw.pmef += __shfl(evdw.pmef, tgx + rlev);
+    eed.pmef  += __shfl_sync(0xffffffff, eed.pmef, tgx + rlev);
+    evdw.pmef += __shfl_sync(0xffffffff, evdw.pmef, tgx + rlev);
 #  endif
   }
   if ((threadIdx.x & GRID_BITS_MASK) == 0) {
diff -Naur a/src/pmemd/src/cuda/kPGS.h b/src/pmemd/src/cuda/kPGS.h
--- a/src/pmemd/src/cuda/kPGS.h	2020-10-01 12:47:20.224767261 +0200
+++ b/src/pmemd/src/cuda/kPGS.h	2020-10-01 13:06:07.189866653 +0200
@@ -274,15 +274,15 @@
     f2 -=  qterm7a * wxdwy1;
     f3 -=  qterm7  * wxwy1  * dtz3;
     unsigned int tx = threadIdx.x & (warpSize - 1);
-    f1 += __shfl(f1, tx ^ 4);
-    f2 += __shfl(f2, tx ^ 4);
-    f3 += __shfl(f3, tx ^ 4);
-    f1 += __shfl(f1, tx ^ 2);
-    f2 += __shfl(f2, tx ^ 2);
-    f3 += __shfl(f3, tx ^ 2);
-    f1 += __shfl(f1, tx ^ 1);
-    f2 += __shfl(f2, tx ^ 1);
-    f3 += __shfl(f3, tx ^ 1);
+    f1 += __shfl_sync(0xffffffff, f1, tx ^ 4);
+    f2 += __shfl_sync(0xffffffff, f2, tx ^ 4);
+    f3 += __shfl_sync(0xffffffff, f3, tx ^ 4);
+    f1 += __shfl_sync(0xffffffff, f1, tx ^ 2);
+    f2 += __shfl_sync(0xffffffff, f2, tx ^ 2);
+    f3 += __shfl_sync(0xffffffff, f3, tx ^ 2);
+    f1 += __shfl_sync(0xffffffff, f1, tx ^ 1);
+    f2 += __shfl_sync(0xffffffff, f2, tx ^ 1);
+    f3 += __shfl_sync(0xffffffff, f3, tx ^ 1);
     if (tgx == 0) {
       sAtom[pos1].fx = f1;
       sAtom[pos1].fy = f2;
